# -*- coding: utf-8 -*-
""Untitled31.ipynb

Automatically generated by Colab.

Original file is located at
    C:/Users/aishu/Downloads/untitled31.py

import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import os

visited = set()

def is_valid_url(url):
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def scrape_site(url, max_depth=1, depth=0):
    global visited
    text_data = ""

    if depth > max_depth or url in visited:
        return text_data

    try:
        response = requests.get(url, timeout=5)
        visited.add(url)
        soup = BeautifulSoup(response.text, "html.parser")
        text_data += f"\n\n---\nğŸ“„ URL: {url}\n\n{text_from_soup(soup)}\n"

        for link in soup.find_all("a", href=True):
            next_url = urljoin(url, link['href'])
            if is_valid_url(next_url):
                text_data += scrape_site(next_url, max_depth, depth + 1)
        time.sleep(1)  # avoid overwhelming the server

    except Exception as e:
        st.warning(f"Failed to scrape {url}: {e}")

    return text_data

def text_from_soup(soup):
    for script in soup(["script", "style", "noscript"]):
        script.decompose()
    return soup.get_text(separator="\n", strip=True)

def save_report(content):
    filename = "scraped_report.txt"
    with open(filename, "w", encoding='utf-8') as file:
        file.write(content)
    return filename

# Streamlit UI
st.title("ğŸŒ Universal Web Scraper with Streamlit")
st.markdown("This tool scrapes any website (including subpages) and generates a downloadable report.")

url_input = st.text_input("Enter a website URL (e.g. https://example.com):")
max_depth = st.slider("Depth of subpages to scrape", 0, 3, 1)

if st.button("Start Scraping"):
    if is_valid_url(url_input):
        visited.clear()
        st.info("Scraping in progress... please wait.")
        scraped_content = scrape_site(url_input, max_depth=max_depth)

        if scraped_content:
            report_file = save_report(scraped_content)
            with open(report_file, "rb") as file:
                st.success("Scraping complete! Download your report below.")
                st.download_button("ğŸ“„ Download Report", file, file_name=report_file)
        else:
            st.warning("No content was scraped.")
    else:
        st.error("Please enter a valid URL.")







